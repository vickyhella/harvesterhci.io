"use strict";(self.webpackChunkharvesterhci_io=self.webpackChunkharvesterhci_io||[]).push([[4950],{3287:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"configure_priority_class_longhorn","metadata":{"permalink":"/kb/configure_priority_class_longhorn","editUrl":"https://github.com/harvester/harvesterhci.io/edit/main/kb/kb/2023-07-25/configure_priority_class_longhorn.md","source":"@site/kb/2023-07-25/configure_priority_class_longhorn.md","title":"Configure PriorityClass on Longhorn System Components","description":"Configure priority classes on Longhorn system components","date":"2023-07-25T00:00:00.000Z","formattedDate":"July 25, 2023","tags":[{"label":"harvester","permalink":"/kb/tags/harvester"},{"label":"longhorn","permalink":"/kb/tags/longhorn"},{"label":"priority class","permalink":"/kb/tags/priority-class"}],"readingTime":6.405,"truncated":false,"authors":[{"name":"Kiefer Chang","title":"Engineer Manager","url":"https://github.com/bk201","image_url":"https://github.com/bk201.png","imageURL":"https://github.com/bk201.png"}],"frontMatter":{"title":"Configure PriorityClass on Longhorn System Components","description":"Configure priority classes on Longhorn system components","slug":"configure_priority_class_longhorn","authors":[{"name":"Kiefer Chang","title":"Engineer Manager","url":"https://github.com/bk201","image_url":"https://github.com/bk201.png","imageURL":"https://github.com/bk201.png"}],"tags":["harvester","longhorn","priority class"],"hide_table_of_contents":false},"nextItem":{"title":"Package your own Toolbox Image","permalink":"/kb/package_your_own_toolbox_image"}},"content":"**Harvester v1.2.0**  introduces a new enhancement where Longhorn system-managed components in newly-deployed clusters are automatically assigned a `system-cluster-critical` priority class by default. However, when upgrading your Harvester clusters from previous versions, you may notice that Longhorn system-managed components do not have any priority class set.\\n\\nThis behavior is intentional and aimed at supporting zero-downtime upgrades. Longhorn does not allow changing the `priority-class` setting when attached volumes exist. For more details, please refer to [Setting Priority Class During Longhorn Installation](https://longhorn.io/docs/1.4.3/advanced-resources/deploy/priority-class/#setting-priority-class-during-longhorn-installation)).\\n\\nThis article explains how to manually configure priority classes for Longhorn system-managed components after upgrading your Harvester cluster, ensuring that your Longhorn components have the appropriate priority class assigned and maintaining the stability and performance of your system.\\n\\n## Stop all virtual machines\\n\\nStop all virtual machines (VMs) to detach all volumes. Please back up any work before doing this.\\n1. [Login to a Harvester controller node and become root](https://docs.harvesterhci.io/v1.1/troubleshooting/os#how-to-log-into-a-harvester-node).\\n2. Get all running VMs and write down their namespaces and names:\\n\\n  ```bash\\n  kubectl get vmi -A\\n  ```\\n\\n  Alternatively, you can get this information by backing up the Virtual Machine Instance (VMI) manifests with the following command:\\n  ```bash\\n  kubectl get vmi -A -o json > vmi-backup.json\\n  ```\\n\\n3. Shut down all VMs. Log in to all running VMs and shut them down gracefully (recommended). Or use the following command to send shutdown signals to all VMs:\\n  ```bash\\n  kubectl get vmi -A -o json | jq -r \'.items[] | [.metadata.name, .metadata.namespace] | @tsv\' | while IFS=$\'\\\\t\' read -r name namespace; do\\n        if [ -z \\"$name\\" ]; then\\n          break\\n        fi\\n        echo \\"Stop ${namespace}/${name}\\"\\n        virtctl stop $name -n $namespace\\n      done\\n  ```\\n\\n  :::note\\n    You can also stop all VMs from the Harvester UI:\\n    1. Go to the **Virtual Machines** page.\\n    2. For each VM, select **\u22ee** > **Stop**.\\n  :::\\n\\n4. Ensure there are no running VMs:\\n\\n  Run the command:\\n\\n  ```bash\\n  kubectl get vmi -A\\n  ```\\n\\n  The above command must return:\\n\\n  ```bash\\n  No resources found\\n\\n## Scale down monitoring pods\\n\\n1. Scale down the Prometheus deployment. Run the following command and wait for all Prometheus pods to terminate:\\n\\n  ```bash\\n  kubectl patch -n cattle-monitoring-system prometheus/rancher-monitoring-prometheus --patch \'{\\"spec\\": {\\"replicas\\": 0}}\' --type merge && \\\\\\n      sleep 5 && \\\\\\n      kubectl rollout status --watch=true -n cattle-monitoring-system statefulset/prometheus-rancher-monitoring-prometheus\\n  ```\\n\\n  A sample output looks like this:\\n\\n  ```\\n  prometheus.monitoring.coreos.com/rancher-monitoring-prometheus patched\\n  statefulset rolling update complete 0 pods at revision prometheus-rancher-monitoring-prometheus-cbf6bd5f7...\\n  ```\\n\\n2. Scale down the AlertManager deployment. Run the following command and wait for all AlertManager pods to terminate:\\n\\n  ```bash\\n  kubectl patch -n cattle-monitoring-system alertmanager/rancher-monitoring-alertmanager --patch \'{\\"spec\\": {\\"replicas\\": 0}}\' --type merge && \\\\\\n      sleep 5 && \\\\\\n      kubectl rollout status --watch=true -n cattle-monitoring-system statefulset/alertmanager-rancher-monitoring-alertmanager\\n  ```\\n\\n  A sample output looks like this:\\n\\n  ```\\n  alertmanager.monitoring.coreos.com/rancher-monitoring-alertmanager patched\\n  statefulset rolling update complete 0 pods at revision alertmanager-rancher-monitoring-alertmanager-c8c459dff...\\n  ```\\n\\n3. Scale down the Grafana deployment. Run the following command and wait for all Grafana pods to terminate:\\n\\n  ```bash\\n  kubectl scale --replicas=0 deployment/rancher-monitoring-grafana -n cattle-monitoring-system && \\\\\\n      sleep 5 && \\\\\\n      kubectl rollout status --watch=true -n cattle-monitoring-system deployment/rancher-monitoring-grafana\\n  ```\\n\\n  A sample output looks like this:\\n\\n  ```\\n  deployment.apps/rancher-monitoring-grafana scaled\\n  deployment \\"rancher-monitoring-grafana\\" successfully rolled out\\n  ```\\n\\n## Scale down vm-import-controller pods\\n\\n1. Check if the [`vm-import-controller`](https://docs.harvesterhci.io/v1.1/advanced/vmimport) addon is enabled and configured with a persistent volume with the following command:\\n\\n  ```bash\\n  kubectl get pvc -n harvester-system harvester-vm-import-controller\\n  ```\\n\\n  If the above command returns an output like this, you must scale down the `vm-import-controller` pod. Otherwise, you can skip the following step.\\n  ```\\n  NAME                             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE\\n  harvester-vm-import-controller   Bound    pvc-eb23e838-4c64-4650-bd8f-ba7075ab0559   200Gi      RWO            harvester-longhorn   2m53s\\n  ```\\n\\n2. Scale down the `vm-import-controller` pods with the following command:\\n\\n  ```bash\\n  kubectl scale --replicas=0 deployment/harvester-vm-import-controller -n harvester-system && \\\\\\n      sleep 5 && \\\\\\n      kubectl rollout status --watch=true -n harvester-system deployment/harvester-vm-import-controller\\n  ```\\n\\n  A sample output looks like this:\\n\\n  ```\\n  deployment.apps/harvester-vm-import-controller scaled\\n  deployment \\"harvester-vm-import-controller\\" successfully rolled out\\n  ```\\n\\n## Set the `priority-class` setting\\n\\n1. Before applying the `priority-class` setting, you need to verify all volumes are detached. Run the following command to verify the `STATE` of each volume is `detached`:\\n\\n  ```bash\\n  kubectl get volumes.longhorn.io -A\\n  ```\\n\\n  Verify the output looks like this:\\n  ```\\n  NAMESPACE         NAME                                       STATE      ROBUSTNESS   SCHEDULED   SIZE           NODE   AGE\\n  longhorn-system   pvc-5743fd02-17a3-4403-b0d3-0e9b401cceed   detached   unknown                  5368709120            15d\\n  longhorn-system   pvc-7e389fe8-984c-4049-9ba8-5b797cb17278   detached   unknown                  53687091200           15d\\n  longhorn-system   pvc-8df64e54-ecdb-4d4e-8bab-28d81e316b8b   detached   unknown                  2147483648            15d\\n  longhorn-system   pvc-eb23e838-4c64-4650-bd8f-ba7075ab0559   detached   unknown                  214748364800          11m\\n  ```\\n\\n1. Set the `priority-class` setting with the following command:\\n\\n  ```bash\\n  kubectl patch -n longhorn-system settings.longhorn.io priority-class --patch \'{\\"value\\": \\"system-cluster-critical\\"}\' --type merge\\n  ```\\n\\n  Longhorn system-managed pods will restart and then you need to check if all the system-managed components have a priority class set:\\n\\n  Get the value of the priority class `system-cluster-critical`:\\n  ```bash\\n  kubectl get priorityclass system-cluster-critical\\n  ```\\n\\n  Verify the output looks like this:\\n  ```\\n  NAME                      VALUE        GLOBAL-DEFAULT   AGE\\n  system-cluster-critical   2000000000   false            15d\\n  ```\\n\\n3. Use the following command to get pods\' priority in the `longhorn-system` namespace:\\n\\n  ```bash\\n  kubectl get pods -n longhorn-system -o custom-columns=\\"Name\\":metadata.name,\\"Priority\\":.spec.priority\\n  ```\\n\\n4. Verify all system-managed components\' pods have the correct priority. System-managed components include:\\n\\n    - `csi-attacher`\\n    - `csi-provisioner`\\n    - `csi-resizer`\\n    - `csi-snapshotter`\\n    - `engine-image-ei`\\n    - `instance-manager-e`\\n    - `instance-manager-r`\\n    - `longhorn-csi-plugin`\\n\\n## Scale up vm-import-controller pods\\n\\nIf you scale down the `vm-import-controller` pods, you must scale it up again. \\n\\n1. Scale up the `vm-import-controller` pod. Run the command: \\n\\n  ```bash\\n  kubectl scale --replicas=1 deployment/harvester-vm-import-controller -n harvester-system && \\\\\\n      sleep 5 && \\\\\\n      kubectl rollout status --watch=true -n harvester-system deployment/harvester-vm-import-controller\\n  ```\\n\\n  A sample output looks like this:\\n\\n  ```\\n  deployment.apps/harvester-vm-import-controller scaled\\n  Waiting for deployment \\"harvester-vm-import-controller\\" rollout to finish: 0 of 1 updated replicas are available...\\n  deployment \\"harvester-vm-import-controller\\" successfully rolled out\\n  ```\\n\\n2. Verify `vm-import-controller` is running using the following command:\\n  ```bash\\n  kubectl get pods --selector app.kubernetes.io/instance=vm-import-controller -A\\n  ```\\n\\n  A sample output looks like this, the pod\'s `STATUS` must be `Running`:\\n  ```\\n  NAMESPACE          NAME                                              READY   STATUS    RESTARTS   AGE\\n  harvester-system   harvester-vm-import-controller-6bd8f44f55-m9k86   1/1     Running   0          4m53s\\n  ```\\n\\n## Scale up monitoring pods\\n\\n1. Scale up the Prometheus deployment. Run the following command and wait for all Prometheus pods to roll out:\\n\\n  ```bash\\n  kubectl patch -n cattle-monitoring-system prometheus/rancher-monitoring-prometheus --patch \'{\\"spec\\": {\\"replicas\\": 1}}\' --type merge && \\\\\\n      sleep 5 && \\\\\\n      kubectl rollout status --watch=true -n cattle-monitoring-system statefulset/prometheus-rancher-monitoring-prometheus\\n  ```\\n\\n  A sample output looks like:\\n  ```\\n  prometheus.monitoring.coreos.com/rancher-monitoring-prometheus patched\\n  Waiting for 1 pods to be ready...\\n  statefulset rolling update complete 1 pods at revision prometheus-rancher-monitoring-prometheus-cbf6bd5f7...\\n  ```\\n\\n2. Scale down the AlertManager deployment. Run the following command and wait for all AlertManager pods to roll out:\\n\\n  ```bash\\n  kubectl patch -n cattle-monitoring-system alertmanager/rancher-monitoring-alertmanager --patch \'{\\"spec\\": {\\"replicas\\": 1}}\' --type merge && \\\\\\n      sleep 5 && \\\\\\n      kubectl rollout status --watch=true -n cattle-monitoring-system statefulset/alertmanager-rancher-monitoring-alertmanager\\n  ```\\n\\n  A sample output looks like this:\\n\\n  ```\\n  alertmanager.monitoring.coreos.com/rancher-monitoring-alertmanager patched\\n  Waiting for 1 pods to be ready...\\n  statefulset rolling update complete 1 pods at revision alertmanager-rancher-monitoring-alertmanager-c8bd4466c...\\n  ```\\n\\n3. Scale down the Grafana deployment. Run the following command and wait for all Grafana pods to roll out:\\n\\n  ```bash\\n  kubectl scale --replicas=1 deployment/rancher-monitoring-grafana -n cattle-monitoring-system && \\\\\\n      sleep 5 && \\\\\\n      kubectl rollout status --watch=true -n cattle-monitoring-system deployment/rancher-monitoring-grafana\\n  ```\\n\\n  A sample output looks like this:\\n\\n  ```\\n  deployment.apps/rancher-monitoring-grafana scaled\\n  Waiting for deployment \\"rancher-monitoring-grafana\\" rollout to finish: 0 of 1 updated replicas are available...\\n  deployment \\"rancher-monitoring-grafana\\" successfully rolled out\\n  ```\\n\\n## Start virtual machines\\n\\n1. Start a VM with the command:\\n\\n  ```bash\\n  virtctl start $name -n $namespace\\n  ```\\n\\n  Replace `$name` with the VM\'s name and `$namespace` with the VM\'s namespace. You can list all virtual machines with the command:\\n\\n  ```bash\\n  kubectl get vms -A\\n  ```\\n\\n  :::note\\n   You can also stop all VMs from the Harvester UI:\\n    1. Go to the **Virtual Machines** page.\\n    2. For each VM, select **\u22ee** > **Start**.\\n  :::\\n\\n  Alternatively, you can start all running VMs with the following command:\\n\\n  ```bash\\n  cat vmi-backup.json | jq -r \'.items[] | [.metadata.name, .metadata.namespace] | @tsv\' | while IFS=$\'\\\\t\' read -r name namespace; do\\n        if [ -z \\"$name\\" ]; then\\n          break\\n        fi\\n        echo \\"Start ${namespace}/${name}\\"\\n        virtctl start $name -n $namespace || true\\n      done\\n  ```"},{"id":"package_your_own_toolbox_image","metadata":{"permalink":"/kb/package_your_own_toolbox_image","editUrl":"https://github.com/harvester/harvesterhci.io/edit/main/kb/kb/2023-07-06/package_your_own_toolbox_image.md","source":"@site/kb/2023-07-06/package_your_own_toolbox_image.md","title":"Package your own Toolbox Image","description":"How to package your own toolbox image","date":"2023-07-06T00:00:00.000Z","formattedDate":"July 6, 2023","tags":[{"label":"debug","permalink":"/kb/tags/debug"},{"label":"harvester","permalink":"/kb/tags/harvester"},{"label":"container","permalink":"/kb/tags/container"}],"readingTime":1.655,"truncated":false,"authors":[{"name":"Vicente Cheng","title":"Senior Software Engineer","url":"https://github.com/Vicente-Cheng","image_url":"https://github.com/Vicente-Cheng.png","imageURL":"https://github.com/Vicente-Cheng.png"}],"frontMatter":{"title":"Package your own Toolbox Image","description":"How to package your own toolbox image","slug":"package_your_own_toolbox_image","authors":[{"name":"Vicente Cheng","title":"Senior Software Engineer","url":"https://github.com/Vicente-Cheng","image_url":"https://github.com/Vicente-Cheng.png","imageURL":"https://github.com/Vicente-Cheng.png"}],"tags":["debug","harvester","container"],"hide_table_of_contents":false},"prevItem":{"title":"Configure PriorityClass on Longhorn System Components","permalink":"/kb/configure_priority_class_longhorn"},"nextItem":{"title":"Scan and Repair Root Filesystem of VirtualMachine","permalink":"/kb/scan-and-repair-vm-root-filesystem"}},"content":"Harvester OS is designed as an immutable operating system, which means you cannot directly install additional packages on it. While there is a way to [install packages](https://docs.harvesterhci.io/dev/troubleshooting/os#how-can-i-install-packages-why-are-some-paths-read-only), it is strongly advised against doing so, as it may lead to system instability.\\n\\nIf you only want to debug with the system, the preferred way is to package the toolbox image with all the needed packages. \\n\\nThis article shares how to package your toolbox image and how to install any packages on the toolbox image that help you debug the system.\\n\\nFor example, if you want to analyze a storage performance issue, you can install `blktrace` on the toolbox image.\\n\\n\\n## Create a Dockerfile\\n\\n```bash\\nFROM opensuse/leap:15.4\\n\\n# Install blktrace\\nRUN zypper in -y \\\\\\n    blktrace\\n\\nRUN zypper clean --all\\n```\\n\\n## Build the image and push\\n```bash\\n# assume you are in the directory of Dockerfile\\n$ docker build -t harvester/toolbox:dev .\\n.\\n.\\n.\\nnaming to docker.io/harvester/toolbox:dev ...\\n$ docker push harvester/toolbox:dev\\n.\\n.\\nd4b76d0683d4: Pushed \\na605baa225e2: Pushed \\n9e9058bdf63c: Layer already exists \\n```\\n\\nAfter you build and push the image, you can run the toolbox using this image to trace storage performance.\\n\\n## Run the toolbox\\n```bash\\n# use `privileged` flag only when you needed. blktrace need debugfs, so I add extra mountpoint.\\ndocker run -it --privileged -v /sys/kernel/debug/:/sys/kernel/debug/ --rm harvester/toolbox:dev bash\\n\\n# test blktrace\\n6ffa8eda3aaf:/ $ blktrace -d /dev/nvme0n1 -o - | blkparse -i -\\n259,0   10     3414     0.020814875 34084  Q  WS 2414127984 + 8 [fio]\\n259,0   10     3415     0.020815190 34084  G  WS 2414127984 + 8 [fio]\\n259,0   10     3416     0.020815989 34084  C  WS 3206896544 + 8 [0]\\n259,0   10     3417     0.020816652 34084  C  WS 2140319184 + 8 [0]\\n259,0   10     3418     0.020817992 34084  P   N [fio]\\n259,0   10     3419     0.020818227 34084  U   N [fio] 1\\n259,0   10     3420     0.020818437 34084  D  WS 2414127984 + 8 [fio]\\n259,0   10     3421     0.020821826 34084  Q  WS 1743934904 + 8 [fio]\\n259,0   10     3422     0.020822150 34084  G  WS 1743934904 + 8 [fio]\\n\\n```"},{"id":"scan-and-repair-vm-root-filesystem","metadata":{"permalink":"/kb/scan-and-repair-vm-root-filesystem","editUrl":"https://github.com/harvester/harvesterhci.io/edit/main/kb/kb/2023-02-01/scan_and_repair_filesystem.md","source":"@site/kb/2023-02-01/scan_and_repair_filesystem.md","title":"Scan and Repair Root Filesystem of VirtualMachine","description":"Scan and repair root filesystem of VM","date":"2023-02-01T00:00:00.000Z","formattedDate":"February 1, 2023","tags":[{"label":"storage","permalink":"/kb/tags/storage"},{"label":"longhorn","permalink":"/kb/tags/longhorn"},{"label":"root","permalink":"/kb/tags/root"},{"label":"filesystem","permalink":"/kb/tags/filesystem"}],"readingTime":2.76,"truncated":false,"authors":[{"name":"Vicente Cheng","title":"Senior Software Engineer","url":"https://github.com/Vicente-Cheng","image_url":"https://github.com/Vicente-Cheng.png","imageURL":"https://github.com/Vicente-Cheng.png"}],"frontMatter":{"title":"Scan and Repair Root Filesystem of VirtualMachine","description":"Scan and repair root filesystem of VM","slug":"scan-and-repair-vm-root-filesystem","authors":[{"name":"Vicente Cheng","title":"Senior Software Engineer","url":"https://github.com/Vicente-Cheng","image_url":"https://github.com/Vicente-Cheng.png","imageURL":"https://github.com/Vicente-Cheng.png"}],"tags":["storage","longhorn","root","filesystem"],"hide_table_of_contents":false},"prevItem":{"title":"Package your own Toolbox Image","permalink":"/kb/package_your_own_toolbox_image"},"nextItem":{"title":"Evicting Replicas From a Disk (the CLI way)","permalink":"/kb/evicting-replicas-from-a-disk-the-cli-way"}},"content":"In earlier versions of Harvester (v1.0.3 and prior), Longhorn volumes may get corrupted during the replica rebuilding process (reference: [Analysis: Potential Data/Filesystem Corruption](https://longhorn.io/kb/troubleshooting-volume-filesystem-corruption/#solution)). In Harvester v1.1.0 and later versions, the Longhorn team has fixed this issue. This article covers manual steps you can take to scan the VM\'s filesystem and repair it if needed.\\n\\n\\n## Stop The VM And Backup Volume\\n\\nBefore you scan the filesystem, it is recommend you back up the volume first. For an example, refer to the following steps to stop the VM and backup the volume.\\n\\n- Find the target VM.\\n\\n![finding the target VM](./imgs/finding_the_target_vm.png)\\n\\n- Stop the target VM.\\n\\n![Stop the target VM](./imgs/stop_the_target_vm.png)\\n\\nThe target VM is stopped and the related volumes are detached. Now go to the Longhorn UI to backup this volume.\\n\\n- Enable `Developer Tools & Features` (Preferences -> Enable Developer Tools & Features).\\n\\n![Preferences then enable developer mode](./imgs/preferences_enable_developer_mode.png)\\n![Enable the developer mode](./imgs/enable_the_developer_mode.png)\\n\\n- Click the `\u22ee` button and select **Edit Config** to edit the config page of the VM.\\n\\n![goto edit config page of VM](./imgs/goto_vm_edit_config_page.png)\\n\\n- Go to the `Volumes` tab and select `Check volume details.`\\n\\n![link to longhorn volume page](./imgs/link_to_longhorn_volume.png)\\n\\n- Click the dropdown menu on the right side and select \'Attach\' to attach the volume again. \\n\\n![attach this volume again](./imgs/attach_this_volume_again.png)\\n\\n- Select the attached node. \\n\\n![choose the attached node](./imgs/choose_the_attached_node.png)\\n\\n- Check the volume attached under `Volume Details` and select `Take Snapshot` on this volume page.\\n\\n![take snapshot on volume page](./imgs/take_snapshot_on_volume_page.png)\\n\\n- Confirm that the snapshot is ready.\\n\\n![check the snapshot is ready](./imgs/check_the_snapshot_is_ready.png)\\n\\nNow that you completed the volume backup, you need to scan and repair the root filesystem.\\n\\n## Scanning the root filesystem and repairing\\n\\nThis section will introduce how to scan the filesystem (e.g., XFS, EXT4) using related tools.\\n\\nBefore scanning, you need to know the filesystem\'s device/partition.\\n\\n- Find the filesystem\'s device by running the `dmesg` command. The most recent device should be what we wanted because we attach the latest.\\n- You should now know the filesystem\'s partition. In the example below, sdd3 is the filesystem\'s partition.\\n\\n![finding the related device](./imgs/finding_related_device.png)\\n\\n- Use the Filesystem toolbox image to scan and repair.\\n\\n```\\n# docker run -it --rm --privileged registry.opensuse.org/isv/rancher/harvester/toolbox/main/fs-toolbox:latest -- bash\\n```\\n\\nThen we try to scan with this target device.\\n\\n### XFS\\n\\nWhen scanning a XFS filesystem, use the `xfs_repair` command as follows, where `/dev/sdd3` is the problematic partition of the device.\\n\\n```\\n# xfs_repair -n /dev/sdd3\\n```\\n\\nTo repair the corrupted partition, run the following command.\\n\\n```\\n# xfs_repair /dev/sdd3\\n```\\n\\n### EXT4\\n\\nWhen scanning a EXT4 filesystem, use the `e2fsck` command as follows, where the `/dev/sde1` is the problematic partition of the device.\\n\\n```\\n# e2fsck -f /dev/sde1\\n```\\n\\nTo repair the corrupted partition, run the following command.\\n\\n```\\n# e2fsck -fp /dev/sde1\\n```\\n\\n\\nAfter using the \'e2fsck\' command, you should also see logs related to scanning and repairing the partition. Scanning and repairing the corrupted partition is successful if there are no errors in these logs. \\n\\n\\n## Detach and Start VM again.\\n\\nAfter the corrupted partition is scanned and repaired, detach the volume and try to start the related VM again.\\n\\n- Detach the volume from the Longhorn UI.\\n\\n![detach volume on longhorn UI](./imgs/detach_volume.png)\\n\\n- Start the related VM again from the Longhorn UI.\\n\\n![Start VM again](./imgs/start_vm_again.png)\\n\\nYour VM should now work normally."},{"id":"evicting-replicas-from-a-disk-the-cli-way","metadata":{"permalink":"/kb/evicting-replicas-from-a-disk-the-cli-way","editUrl":"https://github.com/harvester/harvesterhci.io/edit/main/kb/kb/2023-01-12/evict_replicas_from_a_disk.md","source":"@site/kb/2023-01-12/evict_replicas_from_a_disk.md","title":"Evicting Replicas From a Disk (the CLI way)","description":"Evicting replicas from a disk (the CLI way)","date":"2023-01-12T00:00:00.000Z","formattedDate":"January 12, 2023","tags":[{"label":"storage","permalink":"/kb/tags/storage"},{"label":"longhorn","permalink":"/kb/tags/longhorn"},{"label":"disk","permalink":"/kb/tags/disk"}],"readingTime":1.935,"truncated":false,"authors":[{"name":"Kiefer Chang","title":"Engineer Manager","url":"https://github.com/bk201","image_url":"https://github.com/bk201.png","imageURL":"https://github.com/bk201.png"}],"frontMatter":{"title":"Evicting Replicas From a Disk (the CLI way)","description":"Evicting replicas from a disk (the CLI way)","slug":"evicting-replicas-from-a-disk-the-cli-way","authors":[{"name":"Kiefer Chang","title":"Engineer Manager","url":"https://github.com/bk201","image_url":"https://github.com/bk201.png","imageURL":"https://github.com/bk201.png"}],"tags":["storage","longhorn","disk"],"hide_table_of_contents":false},"prevItem":{"title":"Scan and Repair Root Filesystem of VirtualMachine","permalink":"/kb/scan-and-repair-vm-root-filesystem"},"nextItem":{"title":"NIC Naming Scheme","permalink":"/kb/nic-naming-scheme"}},"content":"Harvester replicates volumes data across disks in a cluster. Before removing a disk, the user needs to evict replicas on the disk to other disks to preserve the volumes\' configured availability. For more information about eviction in Longhorn, please check [Evicting Replicas on Disabled Disks or Nodes](https://longhorn.io/docs/1.3.2/volumes-and-nodes/disks-or-nodes-eviction/).\\n\\n## Preparation\\n\\nThis document describes how to evict Longhorn disks using the `kubectl` command. Before that, users must ensure the environment is set up correctly.\\nThere are two recommended ways to do this:\\n\\n1. Log in to any management node and switch to root (`sudo -i`).\\n1. Download Kubeconfig file and use it locally\\n    - Install `kubectl` and `yq` program manually.\\n    - Open Harvester GUI,  click `support` at the bottom left of the page and click `Download KubeConfig` to download the Kubeconfig file.\\n    - Set the Kubeconfig file\'s path to `KUBECONFIG` environment variable. For example, `export KUBECONFIG=/path/to/kubeconfig`.\\n\\n\\n## Evicting replicas from a disk\\n\\n1. List Longhorn nodes (names are identical to Kubernetes nodes):\\n\\n    ```\\n    kubectl get -n longhorn-system nodes.longhorn.io\\n    ```\\n\\n    Sample output:\\n\\n    ```\\n    NAME    READY   ALLOWSCHEDULING   SCHEDULABLE   AGE\\n    node1   True    true              True          24d\\n    node2   True    true              True          24d\\n    node3   True    true              True          24d\\n    ```\\n\\n1. List disks on a node. Assume we want to evict replicas of a disk on `node1`:\\n\\n    ```\\n    kubectl get -n longhorn-system nodes.longhorn.io node1 -o yaml | yq e \'.spec.disks\'\\n    ```\\n\\n    Sample output:\\n\\n    ```\\n    default-disk-ed7af10f5b8356be:\\n      allowScheduling: true\\n      evictionRequested: false\\n      path: /var/lib/harvester/defaultdisk\\n      storageReserved: 36900254515\\n      tags: []\\n    ```\\n\\n1. Assume disk `default-disk-ed7af10f5b8356be` is the target we want to evict replicas out of.\\n\\n    Edit the node:\\n    ```\\n    kubectl edit -n longhorn-system nodes.longhorn.io node1 \\n    ```\\n\\n    Update these two fields and save:\\n    - `spec.disks.<disk_name>.allowScheduling` to `false`\\n    - `spec.disks.<disk_name>.evictionRequested` to `true`\\n\\n    Sample editing:\\n\\n    ```\\n    default-disk-ed7af10f5b8356be:\\n      allowScheduling: false\\n      evictionRequested: true\\n      path: /var/lib/harvester/defaultdisk\\n      storageReserved: 36900254515\\n      tags: []\\n    ```\\n\\n1. Wait for all replicas on the disk to be evicted.\\n\\n    Get current scheduled replicas on the disk:\\n    ```\\n    kubectl get -n longhorn-system nodes.longhorn.io node1 -o yaml | yq e \'.status.diskStatus.default-disk-ed7af10f5b8356be.scheduledReplica\'\\n    ```\\n\\n    Sample output:\\n    ```\\n    pvc-86d3d212-d674-4c64-b69b-4a2eb1df2272-r-7b422db7: 5368709120\\n    pvc-b06f0b09-f30c-4936-8a2a-425b993dd6cb-r-bb0fa6b3: 2147483648\\n    pvc-b844bcc6-3b06-4367-a136-3909251cb560-r-08d1ab3c: 53687091200\\n    pvc-ea6e0dff-f446-4a38-916a-b3bea522f51c-r-193ca5c6: 10737418240\\n    ```\\n\\n    Run the command repeatedly, and the output should eventually become an empty map:\\n    ```\\n    {}\\n    ```\\n\\n    This means Longhorn evicts replicas on the disk to other disks.\\n\\n    :::note\\n    \\n    If a replica always stays in a disk, please open the [Longhorn GUI](https://docs.harvesterhci.io/v1.1/troubleshooting/harvester#access-embedded-rancher-and-longhorn-dashboards) and check if there is free space on other disks.\\n    :::"},{"id":"nic-naming-scheme","metadata":{"permalink":"/kb/nic-naming-scheme","editUrl":"https://github.com/harvester/harvesterhci.io/edit/main/kb/kb/2022-04-06/nic_naming_scheme.md","source":"@site/kb/2022-04-06/nic_naming_scheme.md","title":"NIC Naming Scheme","description":"NIC Naming Scheme changed after upgrading to v1.0.1","date":"2022-04-06T00:00:00.000Z","formattedDate":"April 6, 2022","tags":[{"label":"network","permalink":"/kb/tags/network"}],"readingTime":1.825,"truncated":false,"authors":[{"name":"Date Huang","title":"Software Engineer","url":"https://github.com/tjjh89017","image_url":"https://github.com/tjjh89017.png","imageURL":"https://github.com/tjjh89017.png"}],"frontMatter":{"title":"NIC Naming Scheme","descripion":"NIC Naming Scheme Change","slug":"nic-naming-scheme","authors":[{"name":"Date Huang","title":"Software Engineer","url":"https://github.com/tjjh89017","image_url":"https://github.com/tjjh89017.png","imageURL":"https://github.com/tjjh89017.png"}],"tags":["network"],"hide_table_of_contents":false},"prevItem":{"title":"Evicting Replicas From a Disk (the CLI way)","permalink":"/kb/evicting-replicas-from-a-disk-the-cli-way"},"nextItem":{"title":"Multiple NICs VM Connectivity","permalink":"/kb/multiple-nics-vm-connectivity"}},"content":"## NIC Naming Scheme changed after upgrading to v1.0.1\\n\\n`systemd` in OpenSUSE Leap 15.3 which is the base OS of Harvester is upgraded to `246.16-150300.7.39.1`. In this version, `systemd` will enable additional naming scheme `sle15-sp3` which is `v238` with `bridge_no_slot`. When there is a PCI bridge associated with NIC, `systemd` will never generate `ID_NET_NAME_SLOT` and naming policy in `/usr/lib/systemd/network/99-default.link` will fallback to `ID_NET_NAME_PATH`. According to this change, NIC names might be changed in your Harvester nodes during the upgrade process from `v1.0.0` to `v1.0.1-rc1` or above, and it will cause network issues that are associated with NIC names.\\n\\n## Effect Settings and Workaround\\n\\n### Startup Network Configuration\\n\\nNIC name changes will need to update the name in `/oem/99_custom.yaml`. You could use [migration script](https://github.com/harvester/upgrade-helpers/blob/main/hack/udev_v238_sle15-sp3.py) to change the NIC names which are associated with a PCI bridge.\\n\\n:::tip\\nYou could find an identical machine to test naming changes before applying the configuration to production machines\\n:::\\n\\nYou could simply execute the script with root account in `v1.0.0` via\\n```bash\\n# python3 udev_v238_sle15-sp3.py\\n```\\n\\nIt will output the patched configuration to the screen and you could compare it to the original one to ensure there is no exception. (e.g. We could use `vimdiff` to check the configuration)\\n```bash\\n# python3 udev_v238_sle15-spe3.py > /oem/test\\n# vimdiff /oem/test /oem/99_custom.yaml\\n```\\n\\nAfter checking the result, we could execute the script with `--really-want-to-do` to override the configuration. It will also back up the original configuration file with a timestamp before patching it.\\n```bash\\n# python3 udev_v238_sle15-sp3.py --really-want-to-do\\n```\\n\\n### Harvester VLAN Network Configuration\\n\\nIf your VLAN network is associated with NIC name directly without `bonding`, you will need to migrate `ClusterNetwork` and `NodeNetwork` with the previous section together.\\n\\n:::note\\nIf your VLAN network is associated with the `bonding` name in `/oem/99_custom.yaml`, you could skip this section.\\n:::\\n\\n#### Modify ClusterNetworks\\n\\nYou need to modify `ClusterNetworks` via \\n```bash\\n$ kubectl edit clusternetworks vlan\\n```\\nsearch this pattern\\n```yaml\\nconfig:\\n  defaultPhysicalNIC: <Your NIC name>\\n```\\nand change to new NIC name\\n\\n#### Modify NodeNetworks\\n\\nYou need to modify `NodeNetworks` via\\n```bash\\n$ kubectl edit nodenetworks <Node name>-vlan\\n```\\nsearch this pattern\\n```yaml\\nspec:\\n  nic: <Your NIC name>\\n```\\nand change to new NIC name"},{"id":"multiple-nics-vm-connectivity","metadata":{"permalink":"/kb/multiple-nics-vm-connectivity","editUrl":"https://github.com/harvester/harvesterhci.io/edit/main/kb/kb/2022-03-10/multiple_nics_vm_connectivity.md","source":"@site/kb/2022-03-10/multiple_nics_vm_connectivity.md","title":"Multiple NICs VM Connectivity","description":"What is the default behavior of a VM with multiple NICs","date":"2022-03-10T00:00:00.000Z","formattedDate":"March 10, 2022","tags":[{"label":"vm","permalink":"/kb/tags/vm"},{"label":"network","permalink":"/kb/tags/network"}],"readingTime":3.955,"truncated":false,"authors":[{"name":"Date Huang","title":"Software Engineer","url":"https://github.com/tjjh89017","image_url":"https://github.com/tjjh89017.png","imageURL":"https://github.com/tjjh89017.png"}],"frontMatter":{"title":"Multiple NICs VM Connectivity","descripion":"How to deal VMs with multiple NICs in Harvester","slug":"multiple-nics-vm-connectivity","authors":[{"name":"Date Huang","title":"Software Engineer","url":"https://github.com/tjjh89017","image_url":"https://github.com/tjjh89017.png","imageURL":"https://github.com/tjjh89017.png"}],"tags":["vm","network"],"hide_table_of_contents":false},"prevItem":{"title":"NIC Naming Scheme","permalink":"/kb/nic-naming-scheme"},"nextItem":{"title":"VM Scheduling","permalink":"/kb/vm-scheduling"}},"content":"## What is the default behavior of a VM with multiple NICs\\n\\nIn [some scenarios](https://github.com/harvester/harvester/issues/1059), you\'ll setup two or more NICs in your VM to serve different networking purposes. If all networks are setup by default with DHCP, you might get random connectivity issues. And while it might get fixed after rebooting the VM, it still will lose connection randomly after some period.\\n\\n## How-to identify connectivity issues\\n\\nIn a Linux VM, you can use commands from the `iproute2` package to identify the default route.\\n\\nIn your VM, execute the following command:\\n```bash\\nip route show default\\n```\\n:::tip\\nIf you get the `access denied` error, please run the command using `sudo`\\n:::\\n    \\nThe output of this command will only show the default route with the gateway and VM IP of the primary network interface (`eth0` in the example below).\\n```\\ndefault via <Gateway IP> dev eth0 proto dhcp src <VM IP> metric 100\\n```\\n\\nHere is the full example:\\n```\\n$ ip route show default\\ndefault via 192.168.0.254 dev eth0 proto dhcp src 192.168.0.100 metric 100\\n```\\n\\nHowever, if the issue covered in this KB occurs, you\'ll only be able to connect to the VM via the VNC or serial console.\\n\\nOnce connected, you can run again the same command as before:\\n```bash\\n$ ip route show default\\n```\\n\\nHowever, this time you\'ll get a default route with an incorrect gateway IP.\\nFor example:\\n```\\ndefault via <Incorrect Gateway IP> dev eth0 proto dhcp src <VM\'s IP> metric 100\\n```\\n\\n## Why do connectivity issues occur randomly\\n\\nIn a standard setup, cloud-based VMs typically use DHCP for their NICs configuration. It will set an IP and a gateway for each NIC. Lastly, a default route to the gateway IP will also be added, so you can use its IP to connect to the VM.\\n\\nHowever, Linux distributions start multiple DHCP clients at the same time and do not have a **priority** system. This means that if you have two or more NICs configured with DHCP, the client will enter a **race condition** to configure the default route. And depending on the currently running Linux distribution DHCP script, there is no guarantee which default route will be configured.\\n\\nAs the default route might change in every DHCP renewing process or after every OS reboot, this will create network connectivity issues.\\n\\n## How to avoid the random connectivity issues\\n\\nYou can easily avoid these connectivity issues by having only one NIC attached to the VM and having only one IP and one gateway configured.\\n\\nHowever, for VMs in more complex infrastructures, it is often not possible to use just one NIC. For example, if your infrastructure has a storage network and a service network. For security reasons, the storage network will be isolated from the service network and have a separate subnet. In this case, you must have two NICs to connect to both the service and storage networks.\\n\\nYou can choose a solution below that meets your requirements and security policy.\\n\\n### Disable DHCP on secondary NIC\\n\\nAs mentioned above, the problem is caused by a `race condition` between two DHCP clients. One solution to avoid this problem is to disable DHCP for all NICs and configure them with static IPs only. Likewise, you can configure the secondary NIC with a static IP and keep the primary NIC enabled with DHCP.\\n\\n1. To configure the primary NIC with a static IP (`eth0` in this example), you can edit the file `/etc/sysconfig/network/ifcfg-eth0` with the following values:\\n\\n```\\nBOOTPROTO=\'static\'\\nIPADDR=\'192.168.0.100\'\\nNETMASK=\'255.255.255.0\'\\n```\\n\\nAlternatively, if you want to reserve the primary NIC using DHCP (`eth0` in this example), use the following values instead:\\n\\n```\\nBOOTPROTO=\'dhcp\'\\nDHCLIENT_SET_DEFAULT_ROUTE=\'yes\'\\n```\\n\\n\\n2. You need to configure the default route by editing the file `/etc/sysconfig/network/ifroute-eth0` (if you configured the primary NIC using DHCP, skip this step):\\n\\n\\n```\\n# Destination  Dummy/Gateway  Netmask  Interface\\ndefault        192.168.0.254  -        eth0\\n```\\n\\n:::warning\\nDo not put other default route for your secondary NIC\\n:::\\n    \\n3. Finally, configure a static IP for the secondary NIC by editing the file `/etc/sysconfig/network/ifcfg-eth1`:\\n\\n```\\nBOOTPROTO=\'static\'\\nIPADDR=\'10.0.0.100\'\\nNETMASK=\'255.255.255.0\'\\n```\\n\\n#### Cloud-Init config\\n\\n```yaml\\nnetwork:\\n  version: 1\\n  config:\\n    - type: physical\\n      name: eth0\\n      subnets:\\n        - type: dhcp\\n    - type: physical\\n      name: eth1\\n      subnets:\\n        - type: static\\n          address: 10.0.0.100/24\\n```\\n   \\n### Disable secondary NIC default route from DHCP\\n\\nIf your secondary NIC requires to get its IP from DHCP, you\'ll need to disable the secondary NIC default route configuration.\\n\\n1. Confirm that the primary NIC configures its default route in the file `/etc/sysconfig/network/ifcfg-eth0`:\\n\\n```\\nBOOTPROTO=\'dhcp\'\\nDHCLIENT_SET_DEFAULT_ROUTE=\'yes\'\\n```\\n\\n2. Disable the secondary NIC default route configuration by editing the file `/etc/sysconfig/network/ifcfg-eth1`:\\n\\n```\\nBOOTPROTO=\'dhcp\'\\nDHCLIENT_SET_DEFAULT_ROUTE=\'no\'\\n```\\n\\n#### Cloud-Init config\\n\\nThis solution is not available in Cloud-Init. Cloud-Init didn\'t allow any option for DHCP."},{"id":"vm-scheduling","metadata":{"permalink":"/kb/vm-scheduling","editUrl":"https://github.com/harvester/harvesterhci.io/edit/main/kb/kb/2022-03-07/vm-scheduling.md","source":"@site/kb/2022-03-07/vm-scheduling.md","title":"VM Scheduling","description":"How does Harvester schedule VMs?","date":"2022-03-07T00:00:00.000Z","formattedDate":"March 7, 2022","tags":[{"label":"vm","permalink":"/kb/tags/vm"},{"label":"scheduling","permalink":"/kb/tags/scheduling"}],"readingTime":15.44,"truncated":false,"authors":[{"name":"PoAn Yang","title":"Software Engineer","url":"https://github.com/FrankYang0529","image_url":"https://github.com/FrankYang0529.png","imageURL":"https://github.com/FrankYang0529.png"}],"frontMatter":{"title":"VM Scheduling","description":"How does Harvester schedule VMs?","slug":"vm-scheduling","authors":[{"name":"PoAn Yang","title":"Software Engineer","url":"https://github.com/FrankYang0529","image_url":"https://github.com/FrankYang0529.png","imageURL":"https://github.com/FrankYang0529.png"}],"tags":["vm","scheduling"],"hide_table_of_contents":false},"prevItem":{"title":"Multiple NICs VM Connectivity","permalink":"/kb/multiple-nics-vm-connectivity"}},"content":"## How does Harvester schedule a VM?\\n\\nHarvester doesn\'t directly schedule a VM in Kubernetes, it relies on [KubeVirt](http://kubevirt.io/) to create the custom resource `VirtualMachine`. When the request to create a new VM is sent, a `VirtualMachineInstance` object is created and it creates the corresponding `Pod`.\\n\\nThe whole VM creation processt leverages `kube-scheduler`, which allows Harvester to use `nodeSelector`, `affinity`, and resources request/limitation to influence where a VM will be deployed.\\n\\n## How does kube-scheduler decide where to deploy a VM?\\n\\nFirst, `kube-scheduler` finds Nodes available to run a pod. After that, `kube-scheduler` scores each available Node by a list of [plugins](https://github.com/kubernetes/kubernetes/tree/v1.22.7/pkg/scheduler/framework/plugins) like [ImageLocality](https://github.com/kubernetes/kubernetes/blob/v1.22.7/pkg/scheduler/framework/plugins/imagelocality/image_locality.go), [InterPodAffinity](https://github.com/kubernetes/kubernetes/tree/v1.22.7/pkg/scheduler/framework/plugins/interpodaffinity), [NodeAffinity](https://github.com/kubernetes/kubernetes/tree/v1.22.7/pkg/scheduler/framework/plugins/nodeaffinity), etc. \\n\\nFinally, `kube-scheduler` calculates the scores from the plugins results for each Node, and select the Node with the highest score to deploy the Pod.\\n\\nFor example, let\'s say  we have a three nodes Harvester cluster with 6 cores CPU and 16G RAM each, and we want to deploy a VM with 1 CPU and 1G RAM (without resources overcommit). \\n\\n`kube-scheduler` will summarize the scores, as displayed in  _Table 1_ below, and will select the node with the highest score, `harvester-node-2` in this case, to deploy the VM.\\n\\n<details>\\n  <summary>kube-scheduler logs</summary>\\n\\n```\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 0,\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 45,\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5350 memory:5941231616] ,score 46,\\n\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 4,\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 34,\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5350 memory:5941231616] ,score 37,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=4\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=34\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=37\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=45\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=46\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" node=\\"harvester-node-0\\" score=1000358\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" node=\\"harvester-node-1\\" score=1000433\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" node=\\"harvester-node-2\\" score=1000437\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm-without-overcommit-75q9b\\", node \\"harvester-node-2\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm-without-overcommit-75q9b\\", node \\"harvester-node-2\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" node=\\"harvester-node-2\\"\\n```\\n</details>\\n\\n**Table 1 - kube-scheduler scores example**\\n\\n|                                 | harvester-node-0 | harvester-node-1 | harvester-node-2 |\\n|:-------------------------------:|:----------------:|:----------------:|:----------------:|\\n| ImageLocality                   |               54 |               54 |               54 |\\n| InterPodAffinity                |                0 |                0 |                0 |\\n| NodeResourcesLeastAllocated     |                4 |               34 |               37 |\\n| NodeAffinity                    |                0 |                0 |                0 |\\n| NodePreferAvoidPods             |          1000000 |          1000000 |          1000000 |\\n| PodTopologySpread               |              200 |              200 |              200 |\\n| TaintToleration                 |              100 |              100 |              100 |\\n| NodeResourcesBalancedAllocation |                0 |               45 |               46 |\\n| Total                           |          1000358 |          1000433 |          1000437 |\\n\\n## Why VMs are distributed unevenly with overcommit?\\n\\nWith resources overcommit, Harvester modifies the resources request. By default, the `overcommit` configuration is `{\\"cpu\\": 1600, \\"memory\\": 150, \\"storage\\":  200}`. This means that if we request a VM with 1 CPU and 1G RAM, its `resources.requests.cpu` will become `62m`. \\n\\n!!! note\\n    The unit suffix `m` stands for \\"thousandth of a core.\\"\\n\\nTo explain it, let\'s take the case of CPU overcommit. The default value of 1 CPU is equal to 1000m CPU, and with the default overcommit configuration of `\\"cpu\\": 1600`, the CPU resource will be 16x smaller. Here is the calculation: `1000m * 100 / 1600 = 62m`.\\n\\nNow, we can see how overcommitting influences `kube-scheduler` scores.\\n\\nIn this example, we use a three nodes Harvester cluster with 6 cores and 16G RAM each. We will deploy two VMs with 1 CPU and 1G RAM, and we will compare the scores for both cases of \\"with-overcommit\\" and \\"without-overcommit\\" resources. \\n\\nThe results of both tables _Table 2_ and _Table 3_ can be explained as follow:\\n\\nIn the \\"with-overcommit\\" case, both VMs are deployed on `harvester-node-2`, however in the \\"without-overcommit\\" case, the VM1 is deployed on `harvester-node-2`, and VM2 is deployed on `harvester-node-1`. \\n\\nIf we look at the detailed scores, we\'ll see a variation of `Total Score` for `harvester-node-2` from `1000459` to `1000461` in the \\"with-overcommit\\" case, and `1000437` to `1000382` in the \\"without-overcommit case\\". It\'s because resources overcommit influences `request-cpu` and `request-memory`. \\n\\nIn the \\"with-overcommit\\" case, the `request-cpu` changes from `4412m` to `4474m`. The difference between the two numbers is `62m`, which is what we calculated above. However, in the \\"without-overcommit\\" case, we send **real** requests to `kube-scheduler`, so the `request-cpu` changes from `5350m` to `6350m`.\\n\\nFinally, since most plugins give the same scores for each node except `NodeResourcesBalancedAllocation` and `NodeResourcesLeastAllocated`, we\'ll see a difference of these two scores for each node.\\n\\nFrom the results, we can see the overcommit feature influences the final score of each Node, so VMs are distributed unevenly. Although the `harvester-node-2` score for VM 2 is higher than VM 1, it\'s not always increasing. In _Table 4_, we keep deploying VM with 1 CPU and 1G RAM, and we can see the score of `harvester-node-2` starts decreasing from 11th VM. The behavior of `kube-scheduler` depends on your cluster resources and the workload you deployed.\\n\\n<details>\\n  <summary>kube-scheduler logs for vm1-with-overcommit</summary>\\n\\n```\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9022 memory:14807289856] ,score 0,\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4622 memory:5992960000] ,score 58,\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4412 memory:5581918208] ,score 59,\\n\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9022 memory:14807289856] ,score 5,\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4622 memory:5992960000] ,score 43,\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4412 memory:5581918208] ,score 46,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=5\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=43\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=46\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=58\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=59\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" node=\\"harvester-node-0\\" score=1000359\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" node=\\"harvester-node-1\\" score=1000455\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" node=\\"harvester-node-2\\" score=1000459\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm1-with-overcommit-ljlmq\\", node \\"harvester-node-2\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm1-with-overcommit-ljlmq\\", node \\"harvester-node-2\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" node=\\"harvester-node-2\\"\\n```\\n</details>\\n\\n<details>\\n  <summary>kube-scheduler logs for vm2-with-overcommit</summary>\\n\\n```\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9022 memory:14807289856] ,score 0,\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4622 memory:5992960000] ,score 58,\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4474 memory:6476701696] ,score 64,\\n\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9022 memory:14807289856] ,score 5,\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4622 memory:5992960000] ,score 43,\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4474 memory:6476701696] ,score 43,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=58\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=64\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=5\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=43\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=43\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" node=\\"harvester-node-0\\" score=1000359\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" node=\\"harvester-node-1\\" score=1000455\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" node=\\"harvester-node-2\\" score=1000461\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm2-with-overcommit-pwrx4\\", node \\"harvester-node-2\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm2-with-overcommit-pwrx4\\", node \\"harvester-node-2\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" node=\\"harvester-node-2\\"\\n```\\n</details>\\n\\n<details>\\n  <summary>kube-scheduler logs for vm1-without-overcommit</summary>\\n\\n```\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 0,\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 45,\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5350 memory:5941231616] ,score 46,\\n\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 4,\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 34,\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5350 memory:5941231616] ,score 37,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=4\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=34\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=37\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=45\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=46\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" node=\\"harvester-node-0\\" score=1000358\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" node=\\"harvester-node-1\\" score=1000433\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" node=\\"harvester-node-2\\" score=1000437\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm1-with-overcommit-6xqmq\\", node \\"harvester-node-2\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm1-with-overcommit-6xqmq\\", node \\"harvester-node-2\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" node=\\"harvester-node-2\\"\\n```\\n</details>\\n\\n<details>\\n  <summary>kube-scheduler logs for vm2-without-overcommit</summary>\\n\\n```\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 0,\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 45,\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:6350 memory:7195328512] ,score 0,\\n\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 4,\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 34,\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:6350 memory:7195328512] ,score 28,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=45\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=4\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=34\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=28\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" node=\\"harvester-node-0\\" score=1000358\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" node=\\"harvester-node-1\\" score=1000433\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" node=\\"harvester-node-2\\" score=1000382\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm2-without-overcommit-mf5vk\\", node \\"harvester-node-1\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm2-without-overcommit-mf5vk\\", node \\"harvester-node-1\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" node=\\"harvester-node-1\\"\\n```\\n</details>\\n\\n**Table 2 - With Overcommit**\\n\\n|              VM 1 / VM 2              |          harvester-node-0 |        harvester-node-1 |        harvester-node-2 |\\n|:-------------------------------------:|--------------------------:|------------------------:|------------------------:|\\n|            request-cpu (m)            |               9022 / 9022 |             4622 / 4622 |             **4412** / **4474** |\\n|             request-memory            | 14807289856 / 14807289856 | 5992960000 / 5992960000 | **5581918208** / **6476701696** |\\n| NodeResourcesBalancedAllocation Score |                     0 / 0 |                 58 / 58 |                 **59** / **64** |\\n|   NodeResourcesLeastAllocated Score   |                     5 / 5 |                 43 / 43 |                 **46** / **43** |\\n| Other Scores                          |         1000354 / 1000354 |       1000354 / 1000354 |       1000354 / 1000354 |\\n|              Total Score              |         1000359 / 1000359 |       1000455 / 1000455 |       **1000459** / **1000461** |\\n\\n**Table 3 - Without Overcommit**\\n\\n|              VM 1 / VM 2              |          harvester-node-0 |        harvester-node-1 |        harvester-node-2 |\\n|:-------------------------------------:|--------------------------:|------------------------:|------------------------:|\\n|            request-cpu (m)            |               9960 / 9960 |             5560 / **5560** |             **5350** / 6350 |\\n|             request-memory            | 15166603264 / 15166603264 | 6352273408 / **6352273408** | **5941231616** / 7195328512 |\\n| NodeResourcesBalancedAllocation Score |                     0 / 0 |                 45 / **45** |                  **46** / 0 |\\n|   NodeResourcesLeastAllocated Score   |                     4 / 4 |                 34 / **34** |                 **37** / 28 |\\n| Other Scores                          |         1000354 / 1000354 |       1000354 / **1000354** |       **1000354** / 1000354 |\\n|              Total Score              |         1000358 / 1000358 |       1000358 / **1000433** |       **1000437** / 1000382 |\\n\\n**Table 4**\\n\\n| Score | harvester-node-0 | harvester-node-1 | harvester-node-2 |\\n|:-----:|-----------------:|-----------------:|-----------------:|\\n|  VM 1 |          1000359 |          1000455 |          1000459 |\\n|  VM 2 |          1000359 |          1000455 |          1000461 |\\n|  VM 3 |          1000359 |          1000455 |          1000462 |\\n|  VM 4 |          1000359 |          1000455 |          1000462 |\\n| VM 5  |          1000359 |          1000455 |          1000463 |\\n|  VM 6 |          1000359 |          1000455 |          1000465 |\\n| VM 7  |          1000359 |          1000455 |          1000466 |\\n| VM 8  |          1000359 |          1000455 |          1000467 |\\n| VM 9  |          1000359 |          1000455 |          1000469 |\\n| VM 10 |          1000359 |          1000455 |          1000469 |\\n| VM 11 |          1000359 |          1000455 |      **1000465** |\\n| VM 12 |          1000359 |          1000455 |      **1000457** |\\n\\n\\n## How to avoid uneven distribution of VMs?\\n\\nThere are many plugins in `kube-scheduler` which we can use to influence the scores. For example, we can add the `podAntiAffinity` plugin to avoid VMs with the same labels being deployed on the same node.\\n\\n```\\n  affinity:\\n    podAntiAffinity:\\n      preferredDuringSchedulingIgnoredDuringExecution:\\n      - podAffinityTerm:\\n          labelSelector:\\n            matchExpressions:\\n            - key: harvesterhci.io/creator\\n              operator: Exists\\n          topologyKey: kubernetes.io/hostname\\n        weight: 100\\n```\\n\\n## How to see scores in kube-scheduler?\\n\\n`kube-scheduler` is deployed as a static pod in Harvester. The file is under `/var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml` in each Management Node. We can add `- --v=10` to the `kube-scheduler` container to show score logs.\\n\\n```\\nkind: Pod\\nmetadata:\\n  labels:\\n    component: kube-scheduler\\n    tier: control-plane\\n  name: kube-scheduler\\n  namespace: kube-system\\nspec:\\n  containers:\\n  - command:\\n    - kube-scheduler\\n    # ...\\n    - --v=10\\n```"}]}')}}]);